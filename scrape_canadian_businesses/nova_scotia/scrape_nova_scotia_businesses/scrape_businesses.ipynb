{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3MwLvC2CFR5p",
   "metadata": {
    "id": "3MwLvC2CFR5p"
   },
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0e821a",
   "metadata": {
    "id": "2e0e821a"
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9Xc1Cs8gFA2x",
   "metadata": {
    "id": "9Xc1Cs8gFA2x"
   },
   "source": [
    "## FUNCTIONAL PROGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd1b7f3-428c-4583-9850-8d1d5f7dbc27",
   "metadata": {
    "id": "4cd1b7f3-428c-4583-9850-8d1d5f7dbc27"
   },
   "outputs": [],
   "source": [
    "def thread(function, webpage_urls):\n",
    "  \"\"\"\n",
    "  speed up the request for html pages\n",
    "  \"\"\"\n",
    "\n",
    "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(function, webpage_urls)\n",
    "\n",
    "def get_tag_contents(webpage_url, tag_name, find_tag, class_selector):\n",
    "    \"\"\"\n",
    "    requests for the html content of the web page.\n",
    "    parse the requested web page content into its component parts\n",
    "    for easy extraction.\n",
    "    locate all the tags with the given tag name and class selector.\n",
    "    extract the text, url from each tag and store it in the \n",
    "    texts, urls list.\n",
    "    \"\"\"\n",
    "    \n",
    "    thread(requests.get, webpage_url)\n",
    "    soup = BeautifulSoup(requests.get(webpage_url).text, \"html.parser\")\n",
    "    \n",
    "    tags = [\n",
    "        tag.find(find_tag, href=True) \n",
    "        for tag in soup.find_all(tag_name, class_=class_selector)\n",
    "    ]\n",
    "    \n",
    "    tag_texts = [tag.text for tag in tags]\n",
    "    tag_urls = [f\"https://yellowpages.ca{tag['href']}\" for tag in tags]\n",
    "    \n",
    "    return tag_texts, tag_urls\n",
    "\n",
    "\n",
    "def get_tags_contents(webpage_urls, tag_name, find_tag, class_selector):\n",
    "    \"\"\"\n",
    "    requests for the html content of the web page.\n",
    "    store the webpage contents in a list.\n",
    "    parse the webpage contents.\n",
    "    store the parsed contents in a list.\n",
    "    extract all texts, urls from the tags at once.\n",
    "    \"\"\"\n",
    "    \n",
    "    thread(requests.get, webpage_urls)\n",
    "    web_contents = [\n",
    "        webpage_content \n",
    "        for webpage_content in (\n",
    "            requests.get(url).text for url in webpage_urls\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    parsed_contents = [\n",
    "        parsed_content \n",
    "        for parsed_content in (\n",
    "            BeautifulSoup(content, \"html.parser\")\n",
    "            for content in web_contents\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    tags_texts = [\n",
    "                  [\n",
    "                      tag.text for tag in [\n",
    "                          tag.find(find_tag, href=True)\n",
    "                          for tag in parsed_content.find_all(\n",
    "                              tag_name, class_=class_selector\n",
    "                          )\n",
    "                      ]\n",
    "                  ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    "\n",
    "    tags_urls = [\n",
    "                  [\n",
    "                      f\"https://yellowpages.ca{tag['href']}\" for tag in [\n",
    "                          tag.find(find_tag, href=True)\n",
    "                          for tag in parsed_content.find_all(\n",
    "                              tag_name, class_=class_selector\n",
    "                          )\n",
    "                      ]\n",
    "                  ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    "\n",
    "    return tags_texts, tags_urls\n",
    "\n",
    "\n",
    "def get_tags_texts(webpage_urls):\n",
    "    \"\"\"\n",
    "    requests for the html content of the web page.\n",
    "    store the webpage contents in a list.\n",
    "    parse the webpage contents.\n",
    "    store the parsed contents in a list.\n",
    "    extract needed texts from the tags at once.\n",
    "    \"\"\"\n",
    "    \n",
    "    thread(requests.get, webpage_urls)\n",
    "    web_contents = [\n",
    "        webpage_content \n",
    "        for webpage_content in (\n",
    "            requests.get(url).text for url in webpage_urls\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    parsed_contents = [\n",
    "        parsed_content \n",
    "        for parsed_content in (\n",
    "            BeautifulSoup(content, \"html.parser\")\n",
    "            for content in web_contents\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    tags_texts_1 = [\n",
    "                  [\n",
    "                      tag.text for tag in [\n",
    "                          tag.find(\"a\", href=True)\n",
    "                          for tag in parsed_content.find_all(\n",
    "                              \"h3\", class_=\"listing__name jsMapBubbleName\"\n",
    "                          )\n",
    "                      ]\n",
    "                  ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    " \n",
    "     \n",
    "    tags_texts_2 = [\n",
    "                 [\n",
    "                     tag.text for tag in [\n",
    "                         tag.find(\"a\", href=True)\n",
    "                         for tag in parsed_content.find_all(\n",
    "                             \"div\", class_=\"listing__headings\"\n",
    "                         )\n",
    "                     ]\n",
    "                 ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    "\n",
    "    tags_texts_3 = [\n",
    "               [    \n",
    "                    tag for tag in [\n",
    "                        tag.find(itemprop=\"streetAddress\")\n",
    "                        for tag in parsed_content.find_all(\n",
    "                            \"div\", class_=\"listing__address address mainLocal noNum\"\n",
    "                        )\n",
    "                    ]\n",
    "                ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    "\n",
    "    tags_texts_4 = [\n",
    "               [    \n",
    "                    tag for tag in [\n",
    "                        tag.find(itemprop=\"addressLocality\")\n",
    "                        for tag in parsed_content.find_all(\n",
    "                            \"div\", class_=\"listing__address address mainLocal noNum\"\n",
    "                        )\n",
    "                    ]\n",
    "                ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    "\n",
    "    tags_texts_5 = [\n",
    "               [    \n",
    "                    tag for tag in [\n",
    "                        tag.find(itemprop=\"addressRegion\")\n",
    "                        for tag in parsed_content.find_all(\n",
    "                            \"div\", class_=\"listing__address address mainLocal noNum\"\n",
    "                        )\n",
    "                    ]\n",
    "                ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    "\n",
    "    tags_texts_6 = [\n",
    "               [    \n",
    "                    tag for tag in [\n",
    "                        tag.find(itemprop=\"postalCode\")\n",
    "                        for tag in parsed_content.find_all(\n",
    "                            \"div\", class_=\"listing__address address mainLocal noNum\"\n",
    "                        )\n",
    "                    ]\n",
    "                ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    "\n",
    "    tags_texts_7 = [\n",
    "               [    \n",
    "                    tag for tag in [\n",
    "                        tag.find(tabindex=\"0\")\n",
    "                        for tag in parsed_content.find_all(\n",
    "                            \"li\", class_=\"mlr__submenu__item\"\n",
    "                        )\n",
    "                    ]\n",
    "                ]\n",
    "        for parsed_content in parsed_contents\n",
    "    ]\n",
    "    \n",
    "    tags_texts = [\n",
    "        [i for texts in tags_texts_1 for i in texts],\n",
    "        [i for texts in tags_texts_2 for i in texts],\n",
    "        [i for texts in tags_texts_3 for i in texts],\n",
    "        [i for texts in tags_texts_4 for i in texts],\n",
    "        [i for texts in tags_texts_5 for i in texts],\n",
    "        [i for texts in tags_texts_6 for i in texts],\n",
    "        [i for texts in tags_texts_7 for i in texts]\n",
    "    ]\n",
    "\n",
    "    texts_titles = [\n",
    "                   \"name\",\n",
    "                   \"type\",\n",
    "                   \"str_addr\",\n",
    "                   \"addr_loc\",\n",
    "                   \"addr_reg\",\n",
    "                   \"post_code\",\n",
    "                   \"phone_num\"\n",
    "    ]\n",
    "    \n",
    "    titles_texts = pd.concat(\n",
    "        [pd.Series(text, name=title) for title, text in zip(\n",
    "            texts_titles, tags_texts)], axis=1)\n",
    "\n",
    "    return titles_texts\n",
    "\n",
    "def extract_bus(text, url):\n",
    "  \"\"\"\n",
    "  extract the bus, bus_type and bus_addr of each location within a \n",
    "  province into a csv file\n",
    "  \"\"\"\n",
    "\n",
    "  thread(get_tag_contents, url)\n",
    "  bus_cats, bus_cat_urls = get_tag_contents(\n",
    "      url, \"h3\", \"a\", \"categories-title catTitle\")\n",
    " \n",
    "  thread(get_tags_contents, bus_cat_urls)\n",
    "  subbus_cat, subbus_cat_urls = get_tags_contents(\n",
    "      bus_cat_urls, \"h3\", \"a\", \"categories-title catTitle\")\n",
    "\n",
    "  subbus_cat_urls = [i for url in subbus_cat_urls for i in url]\n",
    "  thread(get_tags_contents, subbus_cat_urls)\n",
    "  subsubbus_cat, subsubbus_urls = get_tags_contents(\n",
    "      subbus_cat_urls, \"li\", \"a\", \"resp-list\")\n",
    "\n",
    "  subsubbus_urls  = [i for url in subsubbus_urls for i in url]\n",
    "  thread(get_tags_texts, subsubbus_urls)\n",
    "  bus = get_tags_texts(subsubbus_urls)\n",
    "\n",
    "  pd.DataFrame(bus).to_csv(f\"{text.lower().replace(\" \", \"-\")}-businesses.csv\", index=False)\n",
    "\n",
    "  return \"extraction complete!\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "history_visible": true,
   "name": "canadian_businesses_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
